## Read 106: ColPali: Efficient Document Retrieval with Vision Language Models, by @ManuelFaysse, @sibille_hugues, @tonywu_71 et al from Illuin Technology

https://arxiv.org/pdf/2407.01449

The authors of this paper introduce ColPali, a ColBERT-inspired multimodal multi-vector encoder using PaliGemma as a basis for document retrieval. Alongside the model and its intermediary version BiSigLip and BiPali, the authors also release ViDoRe, a document retrieval benchmark targetting specifically document retrieval using visual features.

The authors first focus on the construction of ViDoRe. To do so, they take examples from academic benchmarks; and add domain-specific documents, for which they generate queries using Claude 3 Sonnet, queries which were extensively checked by human annotators (ending with 100 queries for 1000 pages for each domain looked upon). 

This benchmark will be used to compare BiSigLip, BiPali and ColPali with BM25 powered search, BGE-M3 powered search (with documents captioned, ocr-ed or not), and multimodal embedding powered search.

Now, to ColPali overall. ColPali distinguishes itself from the other current embedding methods mainly because it uses multi-vector representations of images and texts, happening respectively at the patch and token levels. The authors exploit ColBERT’s late interaction formula to match the document multi-vector image embeddings (each pdf page being treated as an image) and the query multi-vector text embeddings. This late interaction formula defines the model’s contrastive loss, exploiting positive and negative documents related to a query.

Training dataset is composed of ~120k documents, both from academic datasets (vidore excluded) and a synthetic dataset composed of web pages with queries generated by Sonnet. Additional training details can be found within the paper.

3 models are trained, with ColPali being the final iteration:
1- BiSigLip: Siglip with textual component finetuned on training dataset.
2- BiPali: Take the PaliGemma architecture, take the output patch embeddings and average them into a dense representation (creating thus an encoder). Finetune the model with contrastive loss for the repurposing to work.
3- ColPali: Do not average the output patches into a dense representation, but instead exploit the multivector representation directly for late interaction.

Those approaches are then compared using ViDoRe benchmark. BiSigLip and BiPali are only a mere improvement of SigLip, reaching on average performances lower than fully captioned (or even ocr’d) BM25 or BGE-M3 search. However, ColPali is a step above, reaching very high performances over the benchmark. Vision Encoders aside SigLip are out of the picture, captioning works way better.

It’s worth noting though that due to the large amount of embeddings produced (patch/token-level ones), the embeddings are larger overall and harder to store.

Ablation studies show that having a stronger base LLM makes results comparable with lower patches, that the vision component does not need to be contrastively finetuned, special tokens increase performance for non-english text, pairwise CE loss is better, and that the model can be finetuned for other tasks.

ViDoRe suite can be found on the following huggingface space: https://huggingface.co/vidore

Additional information about the datasets, task results and synthetic data generation can be found within the appendix.

Personal Thoughts: Quite the interesting read! While this is a wonderful application of repurposing a VLM as an encoder, and an amazing use of Colbert-like retrieval, I am unsure on how to use it for large in-production based retrieval. The memory size seems quite limiting. Could it be possible to sparsify the token-embeddings using a specific loss for it? Feeling like it could be a nice boost. Also, would be pretty interested in results leveraging hybrid search for ViDoRe! (BM25+BGE-M3+Reranking)