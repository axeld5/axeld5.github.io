## Read 156: « Deepseek-r1: Incentivizing reasoning capabilities in LLMs via Reinforcement Learning », by @deepseek_ai

Deepseek decided to plunge a big boulder into the open-source lake, which made quite the ripple

They released today, in open-source:
- Deepseek R1 Zero and R1, two very strong reasoning MoE, with performances on par with o1!
- A suite of distilled open source models, from 1.5B to 70B, all stronger than Claude 3.5 Sonnet and Gpt-4o on Maths and Code Generation

How did they pull that off? ⬇️

———-

Let’s go in order and talk first about Deepseek-R1 Zero

The model takes Deepseek v3 as base, and applies GRPO (used to save on the RL costs, according to the authors!) to the model. 

GRPO is applied on reasoning data, with a template structure prompts. Two rewards are given to the model:
1- A first reward that evaluates if it answers well the reasoning question
2- A second reward for formatting: the thinking processed is checked to be kept within <think> brackets

And that’s it! Data sample amount is not mentioned, but this is all that is applied. And the results?

—-

Model reaches o1 performance. Basically, it takes a lot of steps but the model gets there. Impressive! But that’s not the end yet.

Why? Because sadly, Deepseek R1 Zero is not a user friendly model. It struggles with language mixing and poor readability, which means it was up to the authors to fix this. And how did they do?

———

Go back to the base, Deepseek v3.

Collect a data mixture for finetuning: few shot with a long CoT in context, prompting model for detailed answers, straight up taking from Deepseek v3 zero, and refine it thanks to human annotators. A special prompt is used to make sure the model summarizes the reasoning process, so that language and readability remain consistent. Finetune base model on it.

Afterwards, perform RL training the same way it was done from Deepseek r1 zero.

Then, another SFT is done: using reasoning data generated by the model and annotated by Deepseek v3, and non-reasoning data for which CoT is generated when possible using Deepseek v3. This is the only step in the process where the amount of samples is given: 600k (reasoning) + 200k (non-reasoning).

Lastly, another RL process is applied to tune the model: RL on both reasoning and general data. For reasoning data, rewards from before are still applied. For general data, reward models are used to evaluate helpfulness (on output summary), and harmlessness (on every output to identify the risk). Harmlessness is also likely evaluated in reasoning data.

Focusing this part on methodology, and moving on to the results for that Deepseek R1 model

——

Results are basically monstrous. It’s straight up at the level of o1 at every level. For reproducibility, prompt formats were even precised!

Whether it’s code, maths or even knowledge… all I can say is well done. And considering the model is open sourced, hard to cheat with that: the vibe test will tell the truth, in the end.

But it’s not over. Basically the authors finetune smaller models with the 800k samples generated for finetuning in step 3 of Deepseek R1 training.

Only SFT is done in that step. And the results are impressive.

Basically: even the 1.5B model is better than GPT-4o and Claude 3.5 Sonnet on most benchmarks. And the performances just increase by size. Yet another incredible result over there.

——

Authors conclude by stating both PRM and MCTS were ineffective research paths: PRMs have issues handling step splits properly and can lead to reward hacking, while MCTS gets hurt by the tremendous space of token generation when scaling.

Overall, it is yet another work that redraws the map: China caught up. All models are released in an open source license, and as proof of their victory… they released all of the aforementioned models’ weights, and grant permission for generating data through them. This will be a boost for research, that can be sure. The only thing that lacks for reproducibility is the dataset, but I am sure our monster labs will catch up to that eventually.

Very interesting work, which can be found in the following repo: https://github.com/deepseek-ai/DeepSeek-R1/?tab=readme-ov-file