## Read 170: « Cognitive Behaviors that Enable Self-Improving Reasoners, or Four Habits of Highly Effective Stars », by Gandhi et al from Stanford University

https://arxiv.org/pdf/2503.01307

The authors of this paper come after the reasoning-related results (r1, QwQ, distilled-models s1, Limo or Limr) with a simple question: is this mainly a Deepseek/Qwen thing, or can those high reasoning behaviors be also taught through RL with other models? And if they cannot, what makes it hard for them?

To study that, they take 2 3B models : Llama 3.2 and Qwen 2.5.

They take in a very simple environment: the countdown game. You are given a number X and a few other numbers x_i, along the 4 operators (+, x, -, /). The goal is to use the x_i and operators to use X. A very verifiable env, with a reward accorded if the number is gotten using the operators. They then apply PPO to the models.

What they observe is that while Qwen-2.5-3B learns to solve the problem, Llama-3.2-3B does not really make progress, or at least not as fast.

The authors decide to take a look at this issue through the angle of « cognitive behaviors ». They estimate that the reason the models exhibit different learning curves could be because one is able to learn reasoning at a higher level.

They test 4 behaviors:
- Revision of approaches when errors are encountered (backtracking)
- Systematic checking of intermediate results (verification)
- Breaking down a complex problem into manageable steps (subgoal setting)
- Working backwards from the solution to create the intermediary steps (backward chaining)

To check for those behaviors, they generate samples using the base models and use gpt-4-mini to classify the presence of the patterns. This hit as expected: the Qwen 2.5 3B model exhibits often each of those behaviors, while the Llama 3.2 3B and even Llama 3.1 70B exhibit those very weakly!

So now they have this info, they can correct the Llama model ! They do so by generating 7 priming datasets using Countdown problems and Claude 3.5 Sonnet. 5 of those datasets correspond to a combination of strategies, meant to evaluate their relevance. 1 dataset is solution with empty CoT. The last is same amount of tokens as « all strategies enabled », but with only placeholder tokens between the question and answer instead of actual CoT. A deviant dataset of the all strategies version is also created, containing the right reasoning but wrong answers each time.

Results obtained are that the reasoning matters, the answers less. Models trained without CoT or without CoT tokens do not overcome the learning hurdle, while models trained with the strategy tokens manage to learn… even if the outputs are wrong.

The last behavior investigated is  the ability to improve through continued pretraining rather than Finetuning. And the answer is yes! Extract cognitive behaviors from pretraining reasoning math data, construct the set based on the presence of the behaviors, and enrich the model.

Results work once again in the authors’ favor: little to no improvement using the samples without the behaviors, Qwen-level performance using those with the behaviors.

Overall, a truly interesting paper. Qwen models are something else it seems. I wonder if the countdown results do extend to other environments however, but this is already really really intriguing. Data is the model in the end.