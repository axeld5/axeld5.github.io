## Read 33: Better synthetic data by retrieving and transforming existing datasets, by @saumyagandhi007, @ritugala13 et al from Carnegie Mellon University

https://arxiv.org/pdf/2404.14361.pdf

Sometimes, you have problems for which you do not have any data available. And your zero-shot approaches perform badly. This sucks, and this is why you want to generate data. But your model converges too easily. Your examples are mainly alike, and your model cannot generalize properly.

Fighting diversity and bias issues has been quite attempted already, mainly through clever prompting strategies. But what if you could fight it by using already existing datasets, repurposed for your use case? This is what the authors attempt in this paper. They call it DataTune.

Their method, given a user prompt to collect data:
1- Retrieve closest huggingface hub datasets using DataFinder, a bi-encoder specialized for dataset retrieval
2- Use an LLM to rerank the datasets to select the most adapted to the task that needs to be solved. Dataset attributes are included for the reranking, and Self-Consistency is applied to get the best answer
3- Once that dataset is selected, task is given more details and irrelevant columns are removed through two different LLM calls
4- Once those are performed, a planning module is done to transform that dataset into the desired training dataset
5- An execution module is then done to change samples from the origin dataset into a newly, transformed dataset

Their approach is tested over the finetuning of a Mistral-7B on 3000 data points for 6 different tasks. They also compare the performances against few-shot Mistral-7B and GPT-3.5.

While the DataTune method generates way more unique examples than simply prompting the models, it is important to note that the examples generated are, according to GPT-3.5 as judge, much harder than your average example. This is actually unlike the examples generated by simply prompting the models!

In fact, such disparity in distribution means that both methods are compatible. While DataTune alone has performances close to SoTA on the evaluated datasets, it is important to note that DataTune paired with zero-shot generated data has actually higher performances on the benchmarks!

All prompts and experiment details are noted within the papers.

Personal Thoughts: Loved that read! Fun way of adding diverse synthetic data, that paired with the easy to generate samples. Would love to see it paired with other diversity tackling methods, and see how it scales to other use cases!