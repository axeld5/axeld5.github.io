## Read 60: TinyStories: How Small Can Language Models Be and Still Speak Coherent English?, by @EldanRonen and @YuanzhiLi14

https://arxiv.org/pdf/2305.07759

Through all the talk about synthetic data that was once had, let us revisit the origin paper of the phi-models.

In this paper, the authors explore that with a carefully generated dataset, they can make a very small language model be able to tell short stories that are better written than GPT-2XL!

How is TinyStories built? Ask the larger model (here gpt-3.5) to write a simple story, understandable by a 5 years old, having 3 constraints within: a randomly chosen noun, a randomly chosen adjective, and a randomly chosen verb must be part of the story. A variant is also present for which the authors add more constraints, like features (dialogue, bad ending, twist…), a summary of the story and a sentence that should appear somewhere within.

To evaluate models trained on TinyStories, the authors use GPT-4 as a judge, on 5 criteria: Grammar, Creativity, Consistency, Plot Coherence, and Instruction Following in the case in which TinyStories instruct is used for training.

What can be noted is that a 28M 8 layers transformer model solely trained on TinyStories is much better than GPT-2-large, a model that is 20 times its size, at all these metrics. The authors also compare different outputs from different layer sizes, for different types of stories, and show that a small model qualitatively outperforms as well GPT-2-XL, a model a 100 times their size.

The authors then study the models’ generation’s diversity, relatively between themselves and between the origin stories, and find the models perform quite the diverse generation as well.

They also look at interpretability and find that there are both local attention heads (that generate words like « the », « a »…) and semantic attention heads (to generate entities like « Lucy », « banana »…). They find also that some neurons are specifically activated for some grammatical structure, like a neuron that only activates on common nouns or a neuron that only activates on adjectives.

More details about qualitative examples and scaling laws can be found within the paper. TinyStories-Instruct can also be found within Huggingface for people who want to tinker with it.

Personal Thoughts: Really wanted to cover this paper for a while, as this is the one paper that made me enjoy reading research papers. The results blew my mind, and made me want more. A truly great and enjoyable read, which I heavily recommend to anyone who wants to get into synthetic data generation. Here’s to hoping this leads to someone to make an open-phi model ;)