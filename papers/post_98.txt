## Read 98: Scaling Synthetic Data Creation with 1,000,000,000 personas, by Chan et al from Tencent AI Lab Seattle

https://arxiv.org/pdf/2406.20094

The authors of this paper propose a new approach to generate synthetic data, which they apply for good results.

The approach they perform is persona-based. How do they create their persona?

From a text, they extract who is likely to « read | write | like | dislike … » that text, generating through an LLM a persona. They call this the « text-to-persona » approach.

To amplify this text-to-persona approach, they make a persona-to-persona approach, generating from an LLM the persona Y that the persona X would be interacting with in some ways.

In practice, they use GPT-4o on the RedPajama v2 dataset to create their personas. Afterwards, they use word-level and embedding-level deduplication to filter similar ones. This nets them around 1 billion personas.

Once this is done, they test the usefulness of their personas in order to improve an LLM’s performance. They take 1.09M personas and create mathematical problems asking the LM to create them « related to the persona ». GPT-4o is tasked to create the problems, then the solutions. 

Models are then evaluated on both the synthetic data, and most importantly on the Math dataset. The Qwen-2-7B model is compared both in the non-finetuned version, and on the one finetuned on 1.07M synthetic samples. 

What happens is that the Qwen-2 model, finetuned this way, beats 0 shot CoT Claude-3 Opus on the Math Dataset. This is a 15% increase over the base model, which makes overall a very impressive result.

It’s also worth noting that out of 200 problems with solutions generated by Gpt-4o, only 7 were evaluated as invalid, which means the strategy pays off.

The authors also go over other pertinent examples for persona-based synthetic data generation, like writing logic problems, guessing prompts based on a persona, writing knowledge rich texts, game NPCs, or defining an interface…

Synthetic data samples and personas can be found on the following github: https://github.com/tencent-ailab/persona-hub

Personal Thoughts: Simple, yet brilliant and impressive. What’s great about personas is that they can completely throw off next-token prediction, as you can find yourself with a persona widely different from the usual context of studied texts. Generated math problems can find themselves talking about vampires, an old person who loves knitting, or the aztecs. This may be thus one of the strongest leads to beat the curse of diversity that can plague synthetic data gen.