## Research Experiment: Can you use a textual dataset to synthetically create a dataset of another modality, useful in another downstream task?

https://github.com/axeld5/reasoning_charts.git

Dataset: https://huggingface.co/datasets/axel-darmouni/anychart-vqa

## Making the benchmark

📊 First, let me explain where this experience comes from.

Analyzing charts using small models is pain: lots of image concepts intertwine, and charts can be tricky.
Last year, I did SFT a PaliGemma on reasoning data and saw the model often hallucinated on chart understanding.

📝 So what I thought, which I had slightly seen on @IBMResearch literature to some extent, was actually the following: « can I create, from a textual dataset, charts from which I can extract verifiable questions to get verifiable reasoning data? » The answer is yes.

🧮 Let’s take a sentence.
Count letter frequency with a threshold (here 3).
From frequency dict, make bar plot

From that bar plot, you can ask the following:
- How many letters?
- k-th frequency?
- Median frequency?
- Mean frequency?

All those leading to verifiable answers!
⚙️ And we can do the same for pie charts! Except we work in percentages now, but we can still ask the two first questions easily, even if slightly tweaked.

📒 And that is with a sentence, but what about multiple sentences? Let’s create word clouds for instance and ask:
- amount of words?
- k-th most present word?
- taken X word: is word in wordcloud?

📚With this approach, I have created around 300 samples which were answered by Gemini-2.5-Flash. Answers were verified with ground truth, but question was asked using the graph. 2.5-Flash had a pretty great result of higher than 90% valid answers, with very solid reasoning traces!

🔩This was what was done for the competition, which was already quite nice and fun. But once it was over, I wanted to go deeper.
And so, yesterday, booted up an H100 GPU and tested small models on it. Isolated 60 test samples, and benchmarked and tuned models.

## Benchmarking

🔬SmolVLM2-256M-instruct totally whiffs on this dataset taken as a benchmark. 0 instruction following, and answers are usually irrelevant with respect to the question
🔬SmolVLM2-2.2B-instruct is slightly better, with an accuracy of 10% on the bench. This is more often due to the failure in instruction following (ie formatting in \boxed{}) than actually being bad, even though it maxing the IF would not make it shoot that high (I suspect a +20%)
🔬Qwen2.5-3B-VL-Instruct reaches quite acceptable results of nearly 43%. Mainly because of the « word in wordcloud » and « letter count » questions which are easier, yet it is a pretty honest result for a small VLM

## Finetuning

🔧Tuning SmolVLM2-256M on just answer, without reasoning: was quite successful, reached acc of ~37%. Basically, just like Qwen2.5-3B, had an easier time handling the easy questions. Most « is word in wordcloud » are False, and letter count is quite easy.
🔧QLoRA-ing Qwen2.5-3B-VL on reasoning data: success! Model reached around 67% of accuracy, which was unexpected given how much I had to give up due to resource constraints. Was quite impressed, the reasoning traces looked good and the model was able to follow instructions well

## Results

🧩Overall, was a fun challenge, done for the @bespokelabsai competition: netted me a @huggingface Pro subscription, and I think there’s much more to do in terms of curves to generate to improve chart understanding for small models ; like temporal curves, more complex graphics… there’s a lot we can do from only text! One last thanks to @vanstriendaniel for promoting the competition which gave me the final push to go in, and overall was quite a nice fun experience!