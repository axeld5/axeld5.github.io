## Read 134 : Mini Omni: Language Models can hear, talk while thinking in streaming, by @XieZhifei14110 and Wu from Tsinghua University

https://arxiv.org/pdf/2408.16725

The authors present a framework to make a Large Language Model go speech-to-speech.

The architecture they use is similar to VLMs: Whisper is used to encode audio tokens, which are then projected in the LM space. However, to go speech to speech, they apply a specific method.

Their method combines text and audio output tokens into a same vocabulary. It outputs text and speech tokens at the same time, with the audio generated by text-to-speech synthesis. All sequences are summed before producing the next token. Batch parallel decoding is applied to ease audio generation.

Their training methodology is made of three steps:
1- Modality alignment: finetuning both the Audio token Adapter and the TTS Adapter
2- Adaptation training: finetuning the LLM so it is better at the audio handling task
3- Multimodal finetuning: finetuning all but the audio encoder

All training datasets are detailed within the paper. The authors created as well VoiceAssistant-400k using Gpt-4o, a QA dataset composed of 400k entries for speech assistant supervised fine tuning.

The modelâ€™s performances were slightly tested and compared on ASR to whisper small on LibriSpeech test sets, for which it yielded promising results.

Demo can be found on following github repo https://github.com/gpt-omni/mini-omni

Personal Thoughts: A promising work that showcases us the future of speech-to-speech. There can be a big gem if models are scaled here. ;)