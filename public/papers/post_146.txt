## Read 146: Â«Â Mastering Board Games by External and Internal Planning with Large Language ModelsÂ Â», by Schultz, Adamek et al from Google Deepmind

The authors of that paper make 3 contributions :
1- Pretraining a transformer, called Multi-Action Value, based on an LLM architecture that can play board games at a high level
2- Using that transformer as a key tool within an MCTS controller and reaching up to chess Grandmaster levels
3- Distilling that search process into the transformer and reaching a high performance, scaling with search budget

Diving below on each point ðŸ‘‡ 

â€”â€”â€”-

First is the multi-action-value model, shortened as MAV by the authors

This model takes as input:
- Studied game
- The previous state representation
- The previous action
- The Â«Â kÂ Â» amount of legal best moves it should output

The goal of the model is to output:
- The current state representation
- Top k legal actions
- Their action values, ie their win probability
- The best next action to take

Current state representation can also be directly given as input, eliminating the need of inference

The way they compute the action value is actually interesting and not trivial, as they utilize an LLM to represent that

â€”â€”â€”â€”

Actually, win probabilities are quantized! 64 Buckets ranging within [0, 1] are created. 

Therefore, each time the model predicts an action, it outputs afterwards the bucket token, corresponding to the probabilities of that action to be within a bucket!

The bucket tokens can also be used to create actual action values! Two ways were applied by the authors:
1- Max Decoding: Taking the value of the bucket with the highest probability
2- Mean Decoding: Taking values of buckets multiplied by probability of bucket output (ie an expectation over bucket values)

However, there may be cases (basically: surefire win or surefire loss) in which every action belongs to the same bucket

To make sure this is not the case, the authors force the model to output an action. During training, this is put as the action that moves the game forward -a behavior that hopefully is reproduced outside of training-

â€”â€”-

Now regarding training, MAV models are trained using billions of positions and action values from Chess, Chess960, Connect Four and Hex. 

2 models are trained: a 2.7B parameters model, and a 1.1B one. Those models rely on Gemini architecture.

Small but notable detail:
- Models are trained using the game engineâ€™s best choices. That means, for chess, that yes, the Stockfish (16) was used as a teacher.
- Best Outputs of the engine are randomized in order, so that the MAV model does not Â«Â cheatÂ Â» in ordering, nor hallucinate due to false inference of a lexicographical order
- Best action taken is decided as well in training by the game engine

However, this does not mean MAV is gated and will not win over its teacher! Thatâ€™s where interesting things happen: Where does that Multi-Action Value model come in to defeat chess? ;)

â€”â€”â€”-

The first method used by the authors is a revamp of MCTS, called Â«Â external-searchÂ Â».

Basically it amounts to doing Depth-First Searches over the tree of legal moves generated by the MAV. Expansion is guided by an epsilon greedy policy, where the greedy policy is taken as softmax of output moves if action is judged worthy by the MAV, and 0 if action is not.

Action is chosen in the classic MCTS way after tree exploration for a set amount of Depth Searches over the tree, referred to as simulations.

Whatâ€™s peculiarly interesting is that due to the LLM way of doing both state and next action predictions, the need for a game engine for tree exploration is much more reduced

Only intervention is to check if predicted state & actions are valid through a parser that checks for formatting, and marks the tree node as a -inf leaf to prevent exploration if an error is raised

It is relevant as well that to save compute time, the authors did tweak their algorithm into an Â«Â Asynchronous MCTSÂ Â» with batching of evaluations, setting of a timeout and dynamic virtual counts to help in the exploration/exploitation mix

â€”â€”-

The other method introduced uses the previous one: instead of using an MCTS algorithmâ€¦ why not having the MAV be able to perform latently this MCTS?

Basically train the model to perform the expansion on its own, from depth 1-3 and breadth 2-5. Input a prompt, depth and breadth expected, and watch the model try to explore. Context size mattered though, and largest cases (depth 3 and breadth 5) were excluded so that it fits. MAV model was finetuned over 60% MAV data (corresponds to depth 0 Stockfish-annotated) and 40% search data, with over 20k steps of training.

Now, how do the MAV pipes perform?

â€”â€”

The pipes (internal and external searches) are mainly evaluated regarding chess performance. Two elo scores are calculated: internal elo (fighting in bot arenas) and external elo (for chess, extrapolated through perf against stockfish models).

Findings are as follows:
- Both pipelines are very good, higher than Stockfish-L10.
- Internal Search MAV with Breadth=4 and Depth=2 (highest level of exploration wrt context) scores lower than Stockfish-L15, but has an elo close to current SotA of transformer-only model for chess
- MCTS external search MAV with 2000 simulations, on the other handsâ€¦ is at the level of Stockfish-L19/18, being beaten by 20. Reminder: model was trained using Stockfish-L16 scores!
- Scaling simulations (external) / test-time compute (internal) predictably leads to performance increase
- Models show decent performance on Connect 4 and Hex, with External Search still crushing internal.
- Models can generalize to positions that were not present in the training set, which is very interesting to look at :)

â€”â€”-

Overall, a super interesting work
Post took 3 hours to make due to the complexity of the paper, and there are details I glanced over to summarize the key points: more can be found within the paper and its appendix at the following link: https://storage.googleapis.com/deepmind-media/papers/SchultzAdamek24Mastering/SchultzAdamek24Mastering.pdf

Tough to understand, but its use of language to represent game state, and of the MAV model as this massive tool that allows on its own an MCTS-type of exploration is truly worth the look, and I hope may inspire some other works! ;)