## Read 150: « 2.5 years in class: a multimodal textbook for vision-language pretraining », by @spicysweet1859 et al from Zheijang University

https://arxiv.org/pdf/2501.00958

Current existing interleaved datasets, while being rich in examples, still suffer from some caveats in the image text relations.

What happens, in some cases, is that the image is either irrelevant or loosely connected to the text nearby, which is, according to the authors, the room for improvement.

To that end, they use YouTube to extract instructive videos, from which they will create an interleaved image-text dataset containing 6.5 million images for 750 million texts, covering subjects like mathematics, physics or chemistry in extenso.

How do they pull that off? ⬇️

Data is gathered using YouTube’s search API based on a knowledge taxonomy with four hierarchical layers, from subject to course, to sub-course, to knowledge point. 6 scientific subjects are chosen, and the taxonomy is then generated by an LLM.

For each of the 3915 generated knowledge points, the top 50 videos from YouTube are retrieved. They are then deduplicated and a first pass using video metadata is done to filter irrelevant or harmful content. Overall, around 159k videos are collected.

Now, how do the authors go from the videos to the dataset?

——

To create the dataset, they do it in four times:
1- First, ASR transcriptions are extracted using ffmpeg and whisper large. Those transcriptions are then rewritten by Qwen2-72B-Instruct for better fluency and coherence. Videos with low quality ASR are filtered. Only 75k videos remain.
2- ASR are mixed to form coherent paragraphs. Those paragraphs segment the videos into clips. A VideoLlama2 model is used to caption the video clips. The clip captions are compared to the ASR using the gte-Qwen2-7B-instruct embedding model, and the videos with low similarity are discarded -yet the ASR remains.
3- Clip is then split into keyframes, using changes at the image level from frame a to frame b.
4- Two InternVL2-40B are leveraged then to perform OCR on each keyframe to extract key information. Redundant OCR are filtered out.

——

This leads to a dataset of 610k samples containing 6.5M images with 750M text tokens… which represents 250x less than OBELICS, for comparison.

Compared to the others, the dataset contains more image per samples and with higher relations between images of the same sample, as outlined by the  In-Sample Image SIM metrics.

——

The dataset is then tested in 3 ways:
1- Continued pretraining of LLaVa-1.5-7B
2- Continued pretraining of Idefics2-8B
3- Pretraining from scratch a model with Idefics2-8B architecture on this dataset

The dataset is compared to MMC4 and Obelics dataset with sample size taken the same (so only 610k samples used for all this).

At identical sample size, the take-away is clear: this data improves performance compared to the others, and especially consistently improves performance in science-related benchmarks.

——

Another test done by the authors is the « Cheat Test »: can the model answer a question, given the solution as context?

Models trained with MMC4 and OBELICS actually struggle… while models trained with the video textbook dataset actually consistently work pretty well.

——

A few other tests are performed as well, and here are the conclusions:
1- Shuffling the images within the training dataset is very penalizing for training with the authors’ dataset.
2- Models trained using the authors’ dataset can also receive instruction tuning, with better performances than models pretrained with other datasets at equal sample size.

——

Ablations are performed as well to confirm the authors’ full pipe, and several other details like qualitative samples or additional pipeline/experiment informations can be found within the appendix of the paper.

Liking the construction pipeline, it is quite a work of engineering, which is very pleasing to look at! Would love to have the authors’ dataset compared now to the full datasets rather than samples, but the results are quite promising already.

However, I am unsure if YouTube data collection could not pose licence issues when using the dataset to build commercial models. Would love to have a word on this account. Nevertheless, a very nice read!