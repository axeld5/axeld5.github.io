## Read 157: « Vision Language Models do not understand negation », by Alhamoud et al from MIT paired with Google Deepmind

https://negbench.github.io

It can happen in life, when you ask for an image, that you ask for something like « X but not Y », for instance « a cat picture without dogs ».

But the issue is that CLIP models are actually very bad at this retrieval task. To prove it, the authors constructed NegBench, a dataset of captions constructed synthetically.

Their pipeline goes as follows:
1- Extract annotations from image/video datasets
2- Use an LLM to propose an object related to items in the description, that is not in the image
3- Use an item detector to verify presence or absence of said items
4- If absent, keep captions, and generate additional through LLM rephrasing

Authors also construct MCQ-Neg, which is an MCQ version to study alternatively image-text embedding alignment. This version follows three different templates with affirmations, negations or hybrid. For each question, there is a correct answer and three incorrect answer, that can all follow one of the three templates above.

HardNeg-Syn is also created: it is a dataset of 10k image pairs, generated by stable diffusion, where an image contains an object and the other in the pair does not. Said containing is verified with Owl-ViT.

Evaluations reveal that the evaluated CLIP models are really bad at handling negation. Worse than random bad. This also happens in the medical case, which is even more critical.

Basically, what happens after embedding analysis is that the models shortcut. They perform some BoW approach, overlooking the negation words.

Which is why the authors create synthetically samples caption dataset using the CC12M dataset as a starting point. Object extraction (positive and negative) and verification is being performed on each sample, and captions are then rewritten. Llama-3.1 and Owl ViT are allegedly once again used in that step. Two datasets are created, CC12M-NegCap, containing 3 captions for each sample (around 30M captions overall), and CC12M-NegMCQ, containing 4 captions, 1 correct and 3 with 3 hard negative samples. The combination of both is called CC12M-NegFull.

The authors train a CLIP on CC12M-NegFull using a combination of CLIP loss and cross entropy over logits. They see substantial retrieval improvement on the Retrieval Task, and on the MCQ, allowing CLIP to go over random! Worth noting only NegCap is not enough: model does improve retrieval accuracy but not MCQ.

Overall, pretty fun and interesting paper. Result’s cool, but I’d love it if it were tested further:
- Does the same apply to SigLip?
- Most notably, does the same apply to the embeddings of finetuned VLM?

I feel like it should not be the case for the latter, but only experience can tell here. Might give it the test later if I have the time :)