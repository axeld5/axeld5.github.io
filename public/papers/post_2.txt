## ResNet: Deep Residual Learning for Image Recognition

Introduced residual connections that enable training of very deep neural networks.

**Authors**: He et al.  
**Venue**: CVPR 2016

### Summary

This paper addresses the degradation problem in deep neural networks - when networks become deeper, accuracy gets saturated and then degrades rapidly. The authors propose residual learning where layers learn residual functions with reference to the layer inputs.

### Key Innovation

The core idea is the residual block with skip connections:
- Instead of learning H(x), layers learn F(x) = H(x) - x
- The skip connection provides x, so the output is F(x) + x
- This makes it easier to optimize identity mappings

### Impact

- Enabled training of networks with 152+ layers
- Won ImageNet 2015 classification competition
- Residual connections became standard in deep learning
- Influenced countless subsequent architectures

### My Notes

The skip connections solve the vanishing gradient problem elegantly. Still widely used today - you can see residual connections in everything from computer vision to NLP models.

The simplicity of the idea is brilliant. Sometimes the best solutions are the most elegant ones.

### Rating: ⭐⭐⭐⭐

Fundamental contribution that enabled much deeper networks. Essential reading for understanding modern deep learning architectures. 